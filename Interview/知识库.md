# 深度学习

## **转置卷积？**

首先==邻近插值==、==双线性插值==等这类预设的插值方法都是基于人们的先验经验来设计的，在很多场景中效果并不理想，而使用转置卷积的目的是让上采样具有具有可学习的参数，可通过网络学习来获取最优的上采样方式。

在 **填零的输入矩阵 input** 上使用 **经转置的标准卷积核 kernel** 执行 **标准卷积运算**

```python
import torch.nn as nn
import torch
import numpy as np

input = np.random.random((1,1,5,5))
input = torch.tensor(input).float()
# 卷积操作：
# stride:卷积步长
# padding:边缘填充尺寸
# X_o = (X_i + 2p - f)/s +1
Conv = nn.Conv2d(1,1,kernel_size=3, stride=1, padding=1)
# 转置卷积操作步骤：
# 在输入特征图上填充
# 1.边缘填充：padding = k-1-padding
# 2.空洞填充：padding = stride - 1
# 3.对输入特征图填充完毕后，进行正常卷积
# X_o = s(X_i - 1) + f -2p
TranConv = nn.ConvTranspose2d(1,1,kernel_size=3, stride=1, padding=1)
Conv_output = Conv(input)
TranConv_output = TranConv(Conv_output)
print(Conv_output.shape)
print(TranConv_output.shape)
```



## 感受野的计算 - [感受野的理解与计算](https://zhuanlan.zhihu.com/p/113487374)

> **需要理清的概念**
>
> 感受野如何定义？
>
> 如何计算？
>
> 感受野的作用？
>
> 增大感受野的方法？

<img src="https://img-blog.csdnimg.cn/20200505130418516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmdXRlcjIwMTYyMTI4NjI=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

## 空洞卷积

> **产生原因：**分割问题增大感受野用到池化和卷积（下采样），在缩小特征图尺寸再上采样会造成精度损失，所以空洞卷积被提出。
>
> **作用：**在不改变特征图尺寸增大情况下增大感受野。
>
> **过去做法**：Skip Connection。

<img src="https://img-blog.csdnimg.cn/61d1df721ddf4838b0ac53a94a0400cc.png" alt="在这里插入图片描述" style="zoom:50%;" />

```python
import torch.nn as nn
import torch
import numpy as np

input = np.random.random((1,1,5,5))
input = torch.tensor(input).float()
# 空洞卷积：
# 在卷积上填充
# dilation_kernelsize = (k-1)*d+1
Conv = nn.Conv2d(1,1,kernel_size=3, stride=1, padding=1, dilation=2)
Conv_output = Conv(input)
print(Conv_output.shape)
```

## BN层

> **作用：**解决了网络训练过程中”内部协变量偏移“（Internal Covariate Shift）问题
>
> **内部协变量偏移：**每一层的输入特征数据的分布会随着网络训练而发生变化，这是因为网络参数不断变化导致的，这种变化会导致训练困难，因为每一层需要重新适应不同的输入分布
>
> **训练和预测：**训练是对每个mini-batch进行标准化，预测是整个数据集训练后的均值和方差，==均值==为每个mini-batch的均值的平均，==方差==采用每个mini-batch的方差的无偏估
>
> **可学习参数：**γ：缩放因子；β：平移量。因为减去均值除方差未必是最好的分布。
>
> **BN层位置：**非线性激活函数前面，使输入到激活函数的值分布更加稳定
>
> **参数量：**2*C。C表示特征图数量
>
> **优缺点：**优：网络容易训练；缺：依赖于batch大小，batch太小，计算的均值方差不稳定
>
> 



## ResNet - [Resnet残差网络](https://zhuanlan.zhihu.com/p/67860570)

**需要理清的概念**

ResNet解决了一个什么问题？

## Transformer

> Class Token作用？
>
> self-attention?

## VIT- [[详解ViT]]( https://zhuanlan.zhihu.com/p/418184940)

**"Linear Projection of Flattened Patches":**将tokens展平再通过线性层映射到Transformer Encoder的输入纬度

**“CLS”：**Class token embedding，CLS有统计全局信息的能力，用作分类任务的输出。BERT中有两个任务，所以需要CLS，但图像分类任务中CLS不是必须的。

**“Patch + Position Embedding”:**告诉模型tokens之间的相对位置信息。==注意是相加不是contcact==

**"Encoder":**Residual(Layernorm+MultiheadAttention) →Residual(Layernorm+MLP)。==VIT-Encoder与Transformer-Encoder有差别，主要体现在Layernorm位置和MLP替换FW==

​	**"Multihead_Attention":**多个自注意力模块concact。self-Attention的Q，K，V向量由输入向量与QKV矩阵相乘得来。Multihead_Attention则有多组QKV。

​	**“MLP”:**多层线性层（linear,gelu,dropout,linear,dropout）。是前馈神经网络的一种。

**Decoder:**VIT没有Decoder,因为Encoder后直接是MLP→Class

<img src="https://pic4.zhimg.com/80/v2-5afd38bd10b279f3a572b13cda399233_720w.webp" alt="img" style="zoom: 67%;" />

## Swin-Transformer - [Swin Transformer](https://www.bilibili.com/video/BV1vS4y1A7Mu/?spm_id_from=333.788&vd_source=c590aae015e2d7a5953b9e991e27b45c)

> **重要思想:**VIT； Hierarchical； Shift Window
>
> **Hierarachical:**类似于卷积的分层，图片像素随着网络层数增加而减小，通道数增大，感受野增大
>
> **Shift Window:**1.每一个patch内部划分小窗格做注意力来减少计算量；2.用滑动窗口来捕捉特征图的全局关系。做法：将移动窗口外的部分补充到窗口内，做注意力时添加mask来掩盖==非连续部分==的特征关系
>
> 

<img src="https://bbs-img.huaweicloud.com/blogs/img/20211120/1637401566740099562.png" alt="Swin-Transformer介绍-云社区-华为云" style="zoom:67%;" />



# 机器学习 - [机器学习军火库](https://zhuanlan.zhihu.com/p/58434325)

## 监督学习和非监督学习 - [监督/半监督/无监督/弱监督/强化/多示例](https://zhuanlan.zhihu.com/p/45311239)

**需要理清的概念：**

监督学习和非监督学习的区别和概念？（监督学习：回归、分类；监督学习：分类）

什么是弱监督？什么是弱标签？应用场景？

什么是半监督学习？

## LR（Logistic Regression）- [LR逻辑回归模型的原理](https://zhuanlan.zhihu.com/p/151036015)

> LR过程：假设样本服从伯努利分布；通过最大使然估计得到参数估计值；损失函数为凸函数，如果数据满足线性可分、样本足够大、符合模型假设，则为凸问题，有全局最优解，否则为非凸问题梯度下降可能收敛到局部最小值。

**需要理清的概念：**

LR解决什么问题？

什么是伯努利分布？什么是最大使然估计？与LR有什么关系？

LR的假设？

LR掉入局部最小值怎么办？

LR可以用来处理非线性问题么？怎么做？

逻辑回归和线性回归的区别？

逻辑回归和线性回归的损失函数？使用该类损失函数的目的？

## L1和L2正则化 - [什么是 L1 L2 正规化 正则化 ](https://zhuanlan.zhihu.com/p/35356992)

> 正则化是通过在损失函数中加入一个正则化项来对模型的参数进行惩罚。
>
> 正则化可以降低过拟合的程度，主要原因有以下几点：
>
> 参数惩罚：正则化通过在损失函数中引入正则化项，惩罚参数的大小。正则化项通常是参数的范数**乘以**一个正则化系数。这样做可以**鼓励模型选择对非线性影响较小的参数值**，限制参数的增长和波动。过拟合往往是由于**模型过于复杂、参数过多**而导致的，通过参数惩罚可以降低模型的复杂度，减少过拟合的风险。
>
> 模型复杂度控制：正则化可以控制模型的复杂度，平衡模型的拟合能力和泛化能力。过拟合的模型通常在训练数据上表现很好，但在新数据上表现较差。正则化通过限制模型的复杂度，抑制过多地拟合训练数据，降低模型对训练数据的特异性的依赖，从而提高模型的泛化能力。
>
> 特征选择和噪声减少：正则化对于特征选择也很有用。在L1正则化中，正则化项倾向于使得一些不重要的特征对应的参数趋近于零，从而实现了特征选择的效果。通过减少不相关或冗余的特征，正则化可以降低模型对噪声的敏感性，提高模型的鲁棒性和泛化能力。
>
> L1正则化和L2正则化是两种常见的正则化方法：
>
> L1正则化（L1 regularization）：L1正则化通过将参数的**绝对值的和**添加到损失函数中来惩罚参数。这意味着L1正则化鼓励模型的**参数稀疏化，即使得一些参数为零**，从而实现特征选择的效果。L1正则化可以将模型的复杂度降低到最低，并且对异常值和噪声具有一定的鲁棒性。
>
> L2正则化（L2 regularization）：L2正则化通过将参数的**平方和**添加到损失函数中来惩罚参数。L2正则化鼓励模型的**参数趋近于零，但不会直接使参数为零**，因此**不具备特征选择的效果**。L2正则化可以有效地控制模型的复杂度，对异常值和噪声有较好的鲁棒性。
>
> 总结来说，正则化通过参数惩罚和控制模型复杂度，实现了对过拟合的降低。L1正则化和L2正则化是常见的正则化方法，它们在参数惩罚和特征选择方面有所不同。选择哪种正则化方法取决于具体问题和模型的需求。

**需要理清的概念**

什么是正则化？

L1和L2正则化目的？

正则化为什么能降低过拟合程度，并且说明L1正则化和L2正则化？

## 决策树

> 决策树是一种用于分类和回归问题的监督学习模型。它通过一系列的条件判断和决策来对数据进行分类或预测。决策树由节点和边组成，每个节点表示一个条件或问题，每个叶节点表示一个类别或预测值。

根节点该选用哪个特征？接下来呢？如何切分？

什么是熵？(概率越大，熵值越小，不确定性越小)；熵值如何决策一个节点的选择？什么是信息增益？信息增益怎么计算？
$$
H(X) = -	\sum{pi * log{pi}}, i = 1,2,...,n\\
H(X) > 0
$$
如何通过信息增益构造一个决策树？[决策树构造实例](https://www.bilibili.com/video/BV1T84y167U9?p=88&vd_source=c590aae015e2d7a5953b9e991e27b45c)

> ​	先算数据集熵值；遍历计算不同特征下的熵值；计算信息增益（数据集熵值-特征熵值）；取最大的信息增益的特征为当前节点的分割特征

构建决策树的过程？

信息增益选择分割特征的不足？有没有其他选择分割特征的算法？[信息增益率与gini系数](https://www.bilibili.com/video/BV1T84y167U9?p=89&vd_source=c590aae015e2d7a5953b9e991e27b45c)
$$
基尼系数： Gini = \sum_{i=1}^{K} p_i(1-p_i) =1 - \sum_{i=1}^{K} (p_i^2)
$$
决策树为什么要剪纸？什么是预剪枝，有哪些操作？什么是后剪枝，有哪些操作？

决策树做回归任务时选择特征节点的评价指标？

### CART算法



## 集成算法

集成算法有哪些？基本原理是什么？

### Bagging

#### 随机森林

**需要理清的概念**

什么是随机森林？基本原理是什么？优点和缺点？

==分类回归；组合决策树；大数据；缺失异常值；关键特征；不易过拟合==

随机数据采样，随机特征选择分别是什么？为什么要这么做？

==有放回；随机选取；避免过拟合==

随机森林对分类和回归问题如何确定最终的结果？

==投票；多数表决；平均；中位数==

### Boosting

#### 梯度提升决策树（GBDT） - [GBDT(梯度提升决策树)](https://zhuanlan.zhihu.com/p/144855223)

$$
F_{m}(x) = F_{m-1}(x) + argmin_{h}\sum_{i=1}^{n}L(y_{i}, F_{m-1}(x_{i}) + h(x_{i}))\\\\
F_{m}(x):第 m 轮迭代后的强分类器的预测结果\\
h(x_{i}): 弱分类器的预测结果
$$

> 当前预测结果等于前一预测结果与（当前弱分类器预测结果与前一预测结果之和与目标结果的残差）之和

**需要理清的概念**

GBDT的实现细节？包括构建基础？损失函数？训练过程？

==多个弱分类器；纠正之前模型残差；回归：平方损失函数；分类：对数损失函数==

GBDT与CART的关系？

==CART是GBDT的基础==

GBDT的优缺点？

#### 自适应提升算法（AdaBoost（Adaptive Bossting）） - [Adaboost算法原理](https://zhuanlan.zhihu.com/p/41536315)

需要理清的概念：

AdaBoost算法的Adaptive体现在哪里？

简述AdaBoost的执行步骤？

#### 极致梯度提升（XGBoost（eXtreme Gradient Boosting）） - [XGBoost的原理](https://zhuanlan.zhihu.com/p/162001079) - [20道XGBoost面试题](https://mp.weixin.qq.com/s/a4v9n_hUgxNyKSQ3RgDMLA)

> XGBoost的基本思想和GBDT相同，但是做了一些优化，比如二阶导数使损失函数更精准；正则项避免树过拟合；Block存储可以并行计算等。

XGBoost相对于GBDT做了什么优化？XGBoost相对于GBDT的相同和不同点？

什么是XGBoost的目标函数？如何优化目标函数？为什么这样做有用？xgboost为什么要用泰勒展开？

如何做到并行计算的？

> 注意不是tree维度的并行，而是特征维度的并行

### Stacking

**含义：**

> 在构建机器学习模型的过程中，Stacking 能够有效融合多个 Predictor 的结果，是提提高模型分数的重要手段。Stacking 主要含义是，在模型预测得到的结果上，再训练一个模型，就仿佛是在原有的模型上再「堆叠」（Stacking）一个模型。这样做的直觉在于，不同的模型能够提取数据中不同的信息。由于数据的噪声，不同的模型往往会在数据的不同特征上表现很好，但是也各有表现较差的部分。Stacking 就是这样一种方法，它能够把各个模型在提取特征较好的部分给抓取出来，同时舍弃它们各自表现不好的部分，这就能够有效地优化预测结果、提高最终预测的分数了

**核心步骤:**

> 1. 在训练数据上训练并使用多个模型进行预测，得到多组预测结果，也就是我们所说的超特征（Meta Features）；同时我们也对测试数据使用这些模型进行预测，得到测试数据的超特征。
> 2. 使用一个新的模型，对这些超特征再进行训练，训练一个从超特征到真实值（ground-truth）的模型；再将测试数据的超特征输入这些模型，得到最后的结果

**自我总结：**

> 对多个预测模型的预测结果再进行二次预测

## SVM - [支持向量机 SVM](https://zhuanlan.zhihu.com/p/77750026)

**需要理清的概念**

SVM解决什么问题？基本思想是什么？

![【机器学习】支持向量机 SVM（非常详细）](https://pica.zhimg.com/70/v2-e833772fe2044ad9c353fb0173bd0b79_1440w.image?source=172ae18b&biz_tag=Post)

> 我对核函数的理解：可以看作互相关函数，两两数据求内积，将数据之间的关联映射到高维特征，这样非线性的低维特征可能在高维度就线性可分了。==因为这里不需要考虑某个数据的高维特征是什么,只需要考虑数据之间的映射特征关系，所以是一种简化计算==

## 贝叶斯算法

**需要理清的概念**

贝特斯公式和朴素贝叶斯分类？

朴素贝叶斯分类器原理以及公式，出现估计概率值为 0 怎么处理？

> 平滑技术：平滑技术通过在计算条件概率时引入一个小的正数（通常为1），以确保即使在没有观察到的特征值的情况下，概率估计仍不为0。

说说贝叶斯怎么分类啊？比如说看看今天天气怎么样？

## K-means - [K-means（非常详细）](https://zhuanlan.zhihu.com/p/78798251)

> 无监督分类

**需要理清的概念**

K-means算法流程？

优缺点？

优化方向？

## KNN

> 有监督分类

**需要理清的概念**

KNN的基本思想和计算步骤？

# Python

## 迭代器 -（[什么是python的迭代器](https://blog.csdn.net/mpu_nice/article/details/107299963)）

> 迭代的本质：不依靠下标的方式取值，一种通用的方法，是访问集合元素的一种方式。

**需要理清的概念**

什么是迭代器？

什么是可迭代对象？

如何判断一个对象是否是迭代器或可迭代对象？

可迭代对象如何转化为迭代器？

迭代器如何取值？

## 生成器-yield用法 -（[python中yield send的用法详解](https://zhuanlan.zhihu.com/p/363807663)）

> 除了`yield from`之外，`yield`关键字还有其他一些用法和语法。
>
> ```python
> def bb(n):
>     b = (x * x for x in range(n))
>     yield from b
> 
> for x in bb(10):
>     print(x)
> ```
>
> 输出：

> ```
> 0
> 1
> 4
> 9
> 16
> 25
> 36
> 49
> 64
> 81
> ```

基本的`yield`语句：在生成器函数中使用`yield`关键字可以将函数变成一个生成器。它用于产生一个值，并将控制权暂时返回给调用方，然后在下次迭代时继续执行。

```python
def generator():
    yield 1
    yield 2
    yield 3

for value in generator():
    print(value)
```

输出结果为：

```python
1
2
3
```

## 装饰器Decorators - [[Python\]基础语法：函数装饰器](https://zhuanlan.zhihu.com/p/93846887)

> 装饰器允许通过将现有函数传递给装饰器，从而向现有函数添加一些额外的功能，该装饰器将执行现有函数的功能和添加的额外功能。
>
> 装饰器本质上还是一个函数，它可以让已有的函数不做任何改动的情况下增加功能。

## 赋值、浅拷贝和深拷贝 - [Python中的赋值(复制)、浅拷贝与深拷贝](https://zhuanlan.zhihu.com/p/54011712)

**需要理清的概念**

什么是可变对象和不可变对象？

不同类型对象对拷贝有什么影响？

赋值、浅拷贝和深拷贝怎样操作对象的？

浅拷贝的形式有哪些？

深拷贝的形式有哪些？

下面两段代码输出分别是什么？

```python
import copy
l=[1,2,3,[4, 5]]

l1=l #赋值
l2=copy.copy(l) #浅拷贝
l3=copy.deepcopy(l) #深拷贝
l.append(6)

print(l)  
print(l1)
print(l2)
print(l3)

**************************************************************************
import copy
l=[1,2,3,[4, 5]]

l1=l #赋值
l2=copy.copy(l) #浅拷贝
l3=copy.deepcopy(l) #深拷贝
l[3].append(6) 

print(l) 
print(l1)
print(l2)
print(l3)

```

## 垃圾回收机制 - [Python 中的垃圾回收机制](https://zhuanlan.zhihu.com/p/62282961)

> python采用的是**引用计数**机制为主，**标记-清除**和**分代收集（隔代回收）**两种机制为辅的策略。

**需要理清的概念**

引用计数机制的原理？引用计数机制的优缺点？分代回收原理？标记-清楚原理？

## GIL锁 - [Python全局锁(GIL)](https://zhuanlan.zhihu.com/p/639387706)

**需要理清的概念**

什么是GIL锁？

GIL锁出现的目的是什么？跟python的垃圾回收机制有什么关系？

多进程和多线程的区别是什么？

避免GIL限制的方法？

## xrange与range

**需要理清的概念**

xrange与range的区别？

如何使用xrange?

xrange使用场景？

==xrange在python3中被移除，python3中的range结合了xrange与range的功能==

## 列表与元组

**需要理清的概念**

列表与元组是什么类型的对象？可否复制？

列表和元组的使用场景？

## 面向对象 - [C++与Python的面向对象比较](https://zhuanlan.zhihu.com/p/379962554)

**需要理清的概念**

什么是面向对象？

OOP（Object Oriented Programming）的三大特性？

# 计算机网络基础

## TCP/IP - [ TCP三次握手、四次挥手过程及原理](https://zhuanlan.zhihu.com/p/108504297)

**需要理清的概念**

什么是TCP和IP协议，他们有什么关系？TCP/IP是什么？有什么做作用？

TCP与UDP的区别？及应用场景？

描述三次握手？为什么需要三次握手？==（理清客户端和服务端）==

简述四次挥手？为什么需要四次挥手？

## OSI

> ![img](https://pic1.zhimg.com/80/v2-ab12e6f145567d5d4b451bd790c28104_720w.webp)

## 传输协议 - [一文搞懂TCP、IP和HTTP、HTTPS](https://zhuanlan.zhihu.com/p/128000072)

**需要理清的概念**

什么是超文本传输协议？怎么构成的？

# 图像处理基础

## 高斯滤波

**关键字：**平滑图像；减噪；高斯函数；卷积；加权平均；图像细节模糊

## 膨胀腐蚀

**关键字：**白色高亮区域；扩张填充；缩减细化

## 边缘检测

**关键字：**边缘属性；方向；梯度（强度）；一阶导数；二阶导数；最大值；过零点

