{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "代码引用了[Karpathy的ng-video-lecture](https://github.com/karpathy/ng-video-lecture/tree/master)，我在note里面贡献了自己的注释理解，补充了相关理论。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('HarryPotterTxTFile.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  6435489\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chapter : THE BOY WHO LIVED .\n",
            "Mr and Mrs Dursley , of number four , Privet Drive , were proud to say that they were perfectly normal , thank you very much .\n",
            "They were the last people you'd expect to be involved in anything strange or mysterious , because they just didn't hold with such nonsense .\n",
            "Mr Dursley was the director of a firm called Grunnings , which made drills .\n",
            "He was a big , beefy man with hardly any neck , although he did have a very large mustache .\n",
            "Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck , which came in very useful as she spent so much of her time craning over garden fences , spying on the neighbors .\n",
            "The Dursley s had a small son called Dudley and in their opinion there was no finer boy anywhere .\n",
            "The Dursleys had everything they wanted , but they also had a secret , and their greatest fear was that somebody would discover it .\n",
            "They didn't think they could bear it if anyone found out about the Potters .\n",
            "Mrs Potter was Mrs Dursley's \n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        "<br />这里希望得到数据集中的字符集合chars, 以及集合chars的长度vocab_size, vocab_size将作为创建词向量映射哈希表的重要参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !\"&'(),.0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "74\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        "<br />这里encode()返回的是输入字符在基于字符集合chars创建的哈希表中的索引\n",
        "<br />encode()则是求反解，由索引得到字符"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[55, 56, 56, 1, 67, 55, 52, 65, 52]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        "<br /> 将数据集所有字符encode()，并转化为tensor张量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6435489]) torch.int64\n",
            "tensor([24, 55, 48, 63, 67, 52, 65,  1, 20,  1, 41, 29, 26,  1, 23, 36, 46,  1,\n",
            "        44, 29, 36,  1, 33, 30, 43, 26, 25,  1,  9,  0, 34, 65,  1, 48, 61, 51,\n",
            "         1, 34, 65, 66,  1, 25, 68, 65, 66, 59, 52, 72,  1,  8,  1, 62, 53,  1,\n",
            "        61, 68, 60, 49, 52, 65,  1, 53, 62, 68, 65,  1,  8,  1, 37, 65, 56, 69,\n",
            "        52, 67,  1, 25, 65, 56, 69, 52,  1,  8,  1, 70, 52, 65, 52,  1, 63, 65,\n",
            "        62, 68, 51,  1, 67, 62,  1, 66, 48, 72,  1, 67, 55, 48, 67,  1, 67, 55,\n",
            "        52, 72,  1, 70, 52, 65, 52,  1, 63, 52, 65, 53, 52, 50, 67, 59, 72,  1,\n",
            "        61, 62, 65, 60, 48, 59,  1,  8,  1, 67, 55, 48, 61, 58,  1, 72, 62, 68,\n",
            "         1, 69, 52, 65, 72,  1, 60, 68, 50, 55,  1,  9,  0, 41, 55, 52, 72,  1,\n",
            "        70, 52, 65, 52,  1, 67, 55, 52,  1, 59, 48, 66, 67,  1, 63, 52, 62, 63,\n",
            "        59, 52,  1, 72, 62, 68,  5, 51,  1, 52, 71, 63, 52, 50, 67,  1, 67, 62,\n",
            "         1, 49, 52,  1, 56, 61, 69, 62, 59, 69, 52, 51,  1, 56, 61,  1, 48, 61,\n",
            "        72, 67, 55, 56, 61, 54,  1, 66, 67, 65, 48, 61, 54, 52,  1, 62, 65,  1,\n",
            "        60, 72, 66, 67, 52, 65, 56, 62, 68, 66,  1,  8,  1, 49, 52, 50, 48, 68,\n",
            "        66, 52,  1, 67, 55, 52, 72,  1, 57, 68, 66, 67,  1, 51, 56, 51, 61,  5,\n",
            "        67,  1, 55, 62, 59, 51,  1, 70, 56, 67, 55,  1, 66, 68, 50, 55,  1, 61,\n",
            "        62, 61, 66, 52, 61, 66, 52,  1,  9,  0, 34, 65,  1, 25, 68, 65, 66, 59,\n",
            "        52, 72,  1, 70, 48, 66,  1, 67, 55, 52,  1, 51, 56, 65, 52, 50, 67, 62,\n",
            "        65,  1, 62, 53,  1, 48,  1, 53, 56, 65, 60,  1, 50, 48, 59, 59, 52, 51,\n",
            "         1, 28, 65, 68, 61, 61, 56, 61, 54, 66,  1,  8,  1, 70, 55, 56, 50, 55,\n",
            "         1, 60, 48, 51, 52,  1, 51, 65, 56, 59, 59, 66,  1,  9,  0, 29, 52,  1,\n",
            "        70, 48, 66,  1, 48,  1, 49, 56, 54,  1,  8,  1, 49, 52, 52, 53, 72,  1,\n",
            "        60, 48, 61,  1, 70, 56, 67, 55,  1, 55, 48, 65, 51, 59, 72,  1, 48, 61,\n",
            "        72,  1, 61, 52, 50, 58,  1,  8,  1, 48, 59, 67, 55, 62, 68, 54, 55,  1,\n",
            "        55, 52,  1, 51, 56, 51,  1, 55, 48, 69, 52,  1, 48,  1, 69, 52, 65, 72,\n",
            "         1, 59, 48, 65, 54, 52,  1, 60, 68, 66, 67, 48, 50, 55, 52,  1,  9,  0,\n",
            "        34, 65, 66,  1, 25, 68, 65, 66, 59, 52, 72,  1, 70, 48, 66,  1, 67, 55,\n",
            "        56, 61,  1, 48, 61, 51,  1, 49, 59, 62, 61, 51, 52,  1, 48, 61, 51,  1,\n",
            "        55, 48, 51,  1, 61, 52, 48, 65, 59, 72,  1, 67, 70, 56, 50, 52,  1, 67,\n",
            "        55, 52,  1, 68, 66, 68, 48, 59,  1, 48, 60, 62, 68, 61, 67,  1, 62, 53,\n",
            "         1, 61, 52, 50, 58,  1,  8,  1, 70, 55, 56, 50, 55,  1, 50, 48, 60, 52,\n",
            "         1, 56, 61,  1, 69, 52, 65, 72,  1, 68, 66, 52, 53, 68, 59,  1, 48, 66,\n",
            "         1, 66, 55, 52,  1, 66, 63, 52, 61, 67,  1, 66, 62,  1, 60, 68, 50, 55,\n",
            "         1, 62, 53,  1, 55, 52, 65,  1, 67, 56, 60, 52,  1, 50, 65, 48, 61, 56,\n",
            "        61, 54,  1, 62, 69, 52, 65,  1, 54, 48, 65, 51, 52, 61,  1, 53, 52, 61,\n",
            "        50, 52, 66,  1,  8,  1, 66, 63, 72, 56, 61, 54,  1, 62, 61,  1, 67, 55,\n",
            "        52,  1, 61, 52, 56, 54, 55, 49, 62, 65, 66,  1,  9,  0, 41, 55, 52,  1,\n",
            "        25, 68, 65, 66, 59, 52, 72,  1, 66,  1, 55, 48, 51,  1, 48,  1, 66, 60,\n",
            "        48, 59, 59,  1, 66, 62, 61,  1, 50, 48, 59, 59, 52, 51,  1, 25, 68, 51,\n",
            "        59, 52, 72,  1, 48, 61, 51,  1, 56, 61,  1, 67, 55, 52, 56, 65,  1, 62,\n",
            "        63, 56, 61, 56, 62, 61,  1, 67, 55, 52, 65, 52,  1, 70, 48, 66,  1, 61,\n",
            "        62,  1, 53, 56, 61, 52, 65,  1, 49, 62, 72,  1, 48, 61, 72, 70, 55, 52,\n",
            "        65, 52,  1,  9,  0, 41, 55, 52,  1, 25, 68, 65, 66, 59, 52, 72, 66,  1,\n",
            "        55, 48, 51,  1, 52, 69, 52, 65, 72, 67, 55, 56, 61, 54,  1, 67, 55, 52,\n",
            "        72,  1, 70, 48, 61, 67, 52, 51,  1,  8,  1, 49, 68, 67,  1, 67, 55, 52,\n",
            "        72,  1, 48, 59, 66, 62,  1, 55, 48, 51,  1, 48,  1, 66, 52, 50, 65, 52,\n",
            "        67,  1,  8,  1, 48, 61, 51,  1, 67, 55, 52, 56, 65,  1, 54, 65, 52, 48,\n",
            "        67, 52, 66, 67,  1, 53, 52, 48, 65,  1, 70, 48, 66,  1, 67, 55, 48, 67,\n",
            "         1, 66, 62, 60, 52, 49, 62, 51, 72,  1, 70, 62, 68, 59, 51,  1, 51, 56,\n",
            "        66, 50, 62, 69, 52, 65,  1, 56, 67,  1,  9,  0, 41, 55, 52, 72,  1, 51,\n",
            "        56, 51, 61,  5, 67,  1, 67, 55, 56, 61, 58,  1, 67, 55, 52, 72,  1, 50,\n",
            "        62, 68, 59, 51,  1, 49, 52, 48, 65,  1, 56, 67,  1, 56, 53,  1, 48, 61,\n",
            "        72, 62, 61, 52,  1, 53, 62, 68, 61, 51,  1, 62, 68, 67,  1, 48, 49, 62,\n",
            "        68, 67,  1, 67, 55, 52,  1, 37, 62, 67, 67, 52, 65, 66,  1,  9,  0, 34,\n",
            "        65, 66,  1, 37, 62, 67, 67, 52, 65,  1, 70, 48, 66,  1, 34, 65, 66,  1,\n",
            "        25, 68, 65, 66, 59, 52, 72,  5, 66,  1])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        "<br />block_size表示希望模型处理的数据长度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([24, 55, 48, 63, 67, 52, 65,  1, 20])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "n_embd = 128\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        " <br />NLP为一个顺序结果预测，当前位置的预测结果只与前面所有预测结果有关，\n",
        " <br />所以对于tensor([24, 55, 48, 63, 67, 52, 65,  1, 20])这一段数据来说，\n",
        " <br />当输入了24，则希望预测结果是55，\n",
        " <br />当55被成功预测，则[24, 55]参加下一个位置字符的预测，下一个希望预测结果是48,\n",
        " <br />当48被成功预测，则[24, 55, 48]参加下一个位置字符的预测，以此类推，直到达到设定预测字符最大长度\n",
        " <br />下面图片可视化了NLP的预测过程\n",
        " <br /><img src=\"../assert/LLM_predict_process.png\" width=\"40%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([24]) the target: 55\n",
            "when input is tensor([24, 55]) the target: 48\n",
            "when input is tensor([24, 55, 48]) the target: 63\n",
            "when input is tensor([24, 55, 48, 63]) the target: 67\n",
            "when input is tensor([24, 55, 48, 63, 67]) the target: 52\n",
            "when input is tensor([24, 55, 48, 63, 67, 52]) the target: 65\n",
            "when input is tensor([24, 55, 48, 63, 67, 52, 65]) the target: 1\n",
            "when input is tensor([24, 55, 48, 63, 67, 52, 65,  1]) the target: 20\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen**\n",
        "<br />x : 输入张量， y : 目标输出\n",
        "<br />s.shape = [B, T], B : batch_size, GPU并行处理数据的纬度; T : seq_len, 数据的最大上下文长度\n",
        "<br />这里的T理论上越大越好，但需要根据模型的建模能力来设置，因为Transformer的建模能力取决于MHA(Multi-head Attention)的头数，seq_len最长，意味着需要更多的MHA，也就意味着模型越复杂"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[53, 62, 72, 66,  1, 63, 65, 56],\n",
            "        [56, 61, 67, 62,  1, 29, 48, 65],\n",
            "        [ 1,  9,  0,  3,  1, 30,  1, 70],\n",
            "        [51,  1, 62, 69, 52, 65,  1, 48]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[62, 72, 66,  1, 63, 65, 56, 51],\n",
            "        [61, 67, 62,  1, 29, 48, 65, 65],\n",
            "        [ 9,  0,  3,  1, 30,  1, 70, 48],\n",
            "        [ 1, 62, 69, 52, 65,  1, 48, 67]])\n",
            "----\n",
            "when input is [53] the target: 62\n",
            "when input is [53, 62] the target: 72\n",
            "when input is [53, 62, 72] the target: 66\n",
            "when input is [53, 62, 72, 66] the target: 1\n",
            "when input is [53, 62, 72, 66, 1] the target: 63\n",
            "when input is [53, 62, 72, 66, 1, 63] the target: 65\n",
            "when input is [53, 62, 72, 66, 1, 63, 65] the target: 56\n",
            "when input is [53, 62, 72, 66, 1, 63, 65, 56] the target: 51\n",
            "when input is [56] the target: 61\n",
            "when input is [56, 61] the target: 67\n",
            "when input is [56, 61, 67] the target: 62\n",
            "when input is [56, 61, 67, 62] the target: 1\n",
            "when input is [56, 61, 67, 62, 1] the target: 29\n",
            "when input is [56, 61, 67, 62, 1, 29] the target: 48\n",
            "when input is [56, 61, 67, 62, 1, 29, 48] the target: 65\n",
            "when input is [56, 61, 67, 62, 1, 29, 48, 65] the target: 65\n",
            "when input is [1] the target: 9\n",
            "when input is [1, 9] the target: 0\n",
            "when input is [1, 9, 0] the target: 3\n",
            "when input is [1, 9, 0, 3] the target: 1\n",
            "when input is [1, 9, 0, 3, 1] the target: 30\n",
            "when input is [1, 9, 0, 3, 1, 30] the target: 1\n",
            "when input is [1, 9, 0, 3, 1, 30, 1] the target: 70\n",
            "when input is [1, 9, 0, 3, 1, 30, 1, 70] the target: 48\n",
            "when input is [51] the target: 1\n",
            "when input is [51, 1] the target: 62\n",
            "when input is [51, 1, 62] the target: 69\n",
            "when input is [51, 1, 62, 69] the target: 52\n",
            "when input is [51, 1, 62, 69, 52] the target: 65\n",
            "when input is [51, 1, 62, 69, 52, 65] the target: 1\n",
            "when input is [51, 1, 62, 69, 52, 65, 1] the target: 48\n",
            "when input is [51, 1, 62, 69, 52, 65, 1, 48] the target: 67\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[53, 62, 72, 66,  1, 63, 65, 56],\n",
            "        [56, 61, 67, 62,  1, 29, 48, 65],\n",
            "        [ 1,  9,  0,  3,  1, 30,  1, 70],\n",
            "        [51,  1, 62, 69, 52, 65,  1, 48]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Jalen:**\n",
        "> nn.Embedding( ) : 该模块通常用于存储单词嵌入并使用索引检索它们。模块的输入是索引列表，输出是相应的单词嵌入。\n",
        "\n",
        "<br /> 我们需要使用嵌入层将将离散的字符表示为连续的向量表示。\n",
        "<br />例如：\n",
        "\n",
        "| Character | Index | Vector Representation |\n",
        "|-----------|-------|----------------------|\n",
        "|    a      |   0   |     (0.1, 0.2)       |\n",
        "|    b      |   1   |     (0.3, 0.4)       |\n",
        "|    c      |   2   |     (0.5, 0.6)       |\n",
        "|   ...     |  ...  |        ...           |\n",
        "|    z      |   25  |     (0.7, 0.8)       |\n",
        "|  space    |   26  |     (0.9, 1.0)       |\n",
        "\n",
        "<br>\n",
        "\n",
        "> torch.multinomial( ) : 用于从多项分布中进行采样。多项分布是概率分布的一种，它描述了具有多个离散结果的试验或事件的概率分布情况。\n",
        "\n",
        "<br />torch.multinomial函数的返回值是一个张量，其中包含了从给定概率分布中采样得到的离散结果的索引\n",
        "<br />在generate中，我们执行max_new_tokens次，输入的第一个idx为prompt，generate会根据prompt来预测接下来的结果，i次的预测结果取决于i-1次的结果，直到预测结束\n",
        "<br />在生成预测的过程中，我们只需要关注最后一步，即：**logits = logits[:, -1, :]**，因为logits的含义是前i步的所有结果，我们只需要输出第i步的结果，通过torch.multinomial预测出第i个字符索引，并添加到idx中\n",
        "\n",
        ">**实际上transformer在tokenizer过程中，会在词嵌入向量上加上位置编码信息**\n",
        "\n",
        "\n",
        "            def __init__(self, vocab_size):\n",
        "                super().__init__()\n",
        "                # each token directly reads off the logits for the next token from a lookup table\n",
        "                self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "                self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "                self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "            def forward(self, idx, targets=None):\n",
        "                B, T = idx.shape\n",
        "\n",
        "                # idx and targets are both (B,T) tensor of integers\n",
        "                tok_emb = self.token_embedding_table(idx)\n",
        "                pos_emb = self.position_embedding_table(torch.arange(T))\n",
        "                x = tok_emb + pos_emb\n",
        "                logits = self.lm_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 74])\n",
            "tensor(4.7030, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "5Frngn4j\n",
            "5xNA.jBB7h0HYEgpE.(qJyg!v9 NI&g.xBLR38\"PNafM4Yzli3X)rFX5!mrii h5X6CP,C qjSTa.nDZ\"T&)8 wHuxj\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**均方误差（MSE）：** MSE是在回归问题中常用的损失函数，其目标是预测连续输出。它衡量了预测值与实际值之间的平均平方差，通常用于训练回归任务的神经网络。\n",
        "\n",
        "**梯度下降（GD）：** 梯度下降是一种优化算法，用于最小化机器学习模型的损失函数。损失函数衡量了模型根据输入特征对目标变量的预测能力。梯度下降的思想是通过迭代地调整模型参数，沿着损失函数最陡峭的方向进行调整。\n",
        "\n",
        "**动量（Momentum）：** 动量是随机梯度下降（SGD）的一种扩展，它在参数更新中添加了一个“动量”项。这个项有助于平滑更新，并使优化器能够继续朝着正确的方向移动，即使梯度改变方向或变化幅度。动量对于训练深度神经网络特别有用。\n",
        "\n",
        "**RMSprop：** RMSprop是一种优化算法，它使用梯度的平方均值的移动平均来调整每个参数的学习率。这有助于避免参数更新的振荡，并在某些情况下改善收敛性。\n",
        "\n",
        "**Adam：** Adam是一种流行的优化算法，结合了动量和RMSprop的思想。它使用梯度和梯度平方值的移动平均来调整每个参数的学习率。Adam通常被用作深度学习模型的默认优化器。\n",
        "\n",
        "**AdamW：** AdamW是Adam优化器的一种修改版，它在参数更新中添加了权重衰减（weight decay）。这有助于对模型进行正则化，并可以提高泛化性能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.RMSprop(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br />SGD_Loss : 4.783581733703613\n",
        "<br />RMSprop_Loss : 4.534679889678955\n",
        "<br />Adam_Loss : 4.65895938873291\n",
        "<br />AdamW_Loss : 4.65804386138916"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.534679889678955\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ikf8d2XPKMyH4DtoUB 9y7II t\n",
            "a6E2sbiSLh047DuevBDu5Lq6VC.o.xg.aI9\"&FXe6.)YER\n",
            "f1fa:a0zc8BP04?9B,PVH,EMi!sJ3Qbw?:nZNMq6CcvOY?jM:z&(dliePI4y!s8tdwS:zua:Un6OWXPFv&p.HpB6WvHfV\n",
            "\n",
            "MZgqN0RzH8tdE Q0&E09\n",
            "o'w(4E!s7oD x1kFrTnZ4lAihVB2fQS9bj1qejEMCxW,xZ4R0wI'tG:M?B!Vk'dAI8I03:uF\n",
            "y75iSmz.nRDlQQRHuWLTxZNnc8OycC:Pf0nfF?nYy\"8Pwh5LhYo7qJark!(v73 w)D5u7j3ZgXec9&7bODFy hED1\"A\"PN'ueJYYQmb&?mt!?i\"fey97La:vqecK, k!v&C8kx.2vYFy,Aqnx\"AePIgeZ4ifD,zj1yZ?6eZgd9V\n",
            "gaHDZRHu33nX1zlnnhuOGWiSE1qmJ!tBCDX8Ng3 \"AUNI LFSUvuB,zwHuHsJp\"q7\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jalen:\n",
        "import time\n",
        "class code_process_time:\n",
        "    def start(self):\n",
        "        self.start_time = time.time()\n",
        "    def end(self):\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - self.start_time\n",
        "        execution_time = end_time - self.start_time\n",
        "\n",
        "        print(\"代码执行时间：\", execution_time, \"秒\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面的代码中，展示了一种数学技巧 ———— 使用下三角矩阵，用叉乘方法计算b的前i行的均值 来代替 循环计算b的前i行的均值。这明显效率更高。\n",
        "<br />在下面这里例子中，你可以看到，c和d的值是一样的，只不过他们用了不同的计算方式\n",
        "<br />下面结果可以看到，第一种方法运算效率高于第二种"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tril_a=\n",
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
            "--\n",
            "代码执行时间： 0.0009992122650146484 秒\n",
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7., 6.,  ..., 1., 6., 4.],\n",
            "        [6., 9., 4.,  ..., 3., 9., 0.],\n",
            "        [9., 5., 8.,  ..., 8., 7., 3.],\n",
            "        ...,\n",
            "        [9., 0., 6.,  ..., 7., 4., 3.],\n",
            "        [9., 4., 2.,  ..., 5., 5., 5.],\n",
            "        [1., 1., 3.,  ..., 9., 2., 4.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000, 6.0000,  ..., 1.0000, 6.0000, 4.0000],\n",
            "        [4.0000, 8.0000, 5.0000,  ..., 2.0000, 7.5000, 2.0000],\n",
            "        [5.6667, 7.0000, 6.0000,  ..., 4.0000, 7.3333, 2.3333],\n",
            "        ...,\n",
            "        [4.5604, 4.5403, 4.7752,  ..., 4.3691, 4.3423, 4.5302],\n",
            "        [4.5753, 4.5385, 4.7659,  ..., 4.3712, 4.3445, 4.5318],\n",
            "        [4.5633, 4.5267, 4.7600,  ..., 4.3867, 4.3367, 4.5300]])\n",
            "--\n",
            "代码执行时间： 0.017000436782836914 秒\n",
            "d=\n",
            "tensor([[2.0000, 7.0000, 6.0000,  ..., 1.0000, 6.0000, 4.0000],\n",
            "        [4.0000, 8.0000, 5.0000,  ..., 2.0000, 7.5000, 2.0000],\n",
            "        [5.6667, 7.0000, 6.0000,  ..., 4.0000, 7.3333, 2.3333],\n",
            "        ...,\n",
            "        [4.5604, 4.5403, 4.7752,  ..., 4.3691, 4.3423, 4.5302],\n",
            "        [4.5753, 4.5385, 4.7659,  ..., 4.3712, 4.3445, 4.5318],\n",
            "        [4.5633, 4.5267, 4.7600,  ..., 4.3867, 4.3367, 4.5300]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "cal_time = code_process_time()\n",
        "# Jalen:\n",
        "# 使用下三角矩阵，用叉乘方法计算b的前i行的均值，并保存到c中\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(300, 300)) # 获取一个张量的下三角部分\n",
        "print('tril_a=')\n",
        "print(a)\n",
        "print('--')\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(300,200)).float()\n",
        "cal_time.start()\n",
        "c = a @ b\n",
        "cal_time.end()\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "print('--')\n",
        "\n",
        "\n",
        "# Jalen:\n",
        "# 循环计算b的前i行的均值，并保存到d中\n",
        "d = torch.zeros((b.shape[0],b.shape[1]))\n",
        "cal_time.start()\n",
        "for i in range(b.shape[0]):\n",
        "    d[i] = (torch.mean(b[:i+1], dim=0))\n",
        "cal_time.end()\n",
        "print('d=')\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**法一：循环计算权重累积**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        \n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**法二：矩阵叉乘计算权重累积**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**法三：masked_fill + softmax**\n",
        "<br />这里用masked_fill + softmax代替了法二的mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**将法三用到self-attention的计算当中**\n",
        "<br />还有值得说的一点是，这里将等于是mask(softmax(Q,K))，这样做的目的是因为nlp是线性预测任务，我们不希望前面的字符注意到没有预测出来的字符。\n",
        "<br />这在LLAMA2的结构里也有同样的操作\n",
        "\n",
        "> 下面的图为LLAMA2的结构，可以看到softmax(Q,K)也有一个mask操作\n",
        "> \n",
        "<img src=\"../assert/LLAMA2.png\" width=\"40%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
            "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
            "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
            "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
            "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
            "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
            "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这里展示了为什么Attention机制需要<code>wei = q @ k.transpose(-2, -1) * head_size**-0.5</code>,即除以一个d_k\n",
        "<br />原因是softmax会锐化较大的值,可以看到上面的输出结果峰值比较平均，而下面的输出峰值明显\n",
        ">Softmax公式如下：\n",
        "$$\n",
        "softmax(x_i) = \\frac{e^{x_i}}{\\sum^n_{i=0}e^{x_i}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面的代码展示了Laynorm的构成细节\n",
        "<br />并且这里和LLAMA2不同的是，LLAMA2使用的是RMSNorm\n",
        ">LayerNorm公式如下：\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\gamma + \\beta\n",
        "\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "## Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though.\n",
        "\n",
        "**从下面的代码你可以看到，NanoGPT是一个decoder-only的结构**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.21089 M parameters\n",
            "step 0: train loss 4.4421, val loss 4.4432\n",
            "step 100: train loss 2.5018, val loss 2.4826\n",
            "step 200: train loss 2.3833, val loss 2.3681\n",
            "step 300: train loss 2.3010, val loss 2.2799\n",
            "step 400: train loss 2.2375, val loss 2.2134\n",
            "step 500: train loss 2.1652, val loss 2.1483\n",
            "step 600: train loss 2.1038, val loss 2.0803\n",
            "step 700: train loss 2.0476, val loss 2.0402\n",
            "step 800: train loss 2.0060, val loss 2.0060\n",
            "step 900: train loss 1.9575, val loss 1.9550\n",
            "step 1000: train loss 1.9264, val loss 1.9143\n",
            "step 1100: train loss 1.8898, val loss 1.8812\n",
            "step 1200: train loss 1.8629, val loss 1.8555\n",
            "step 1300: train loss 1.8384, val loss 1.8170\n",
            "step 1400: train loss 1.8093, val loss 1.8080\n",
            "step 1500: train loss 1.7966, val loss 1.8017\n",
            "step 1600: train loss 1.7681, val loss 1.7757\n",
            "step 1700: train loss 1.7696, val loss 1.7655\n",
            "step 1800: train loss 1.7416, val loss 1.7554\n",
            "step 1900: train loss 1.7344, val loss 1.7307\n",
            "step 2000: train loss 1.7111, val loss 1.7221\n",
            "step 2100: train loss 1.6964, val loss 1.6999\n",
            "step 2200: train loss 1.6883, val loss 1.7016\n",
            "step 2300: train loss 1.6836, val loss 1.6899\n",
            "step 2400: train loss 1.6675, val loss 1.6744\n",
            "step 2500: train loss 1.6645, val loss 1.6623\n",
            "step 2600: train loss 1.6582, val loss 1.6631\n",
            "step 2700: train loss 1.6498, val loss 1.6549\n",
            "step 2800: train loss 1.6442, val loss 1.6477\n",
            "step 2900: train loss 1.6216, val loss 1.6462\n",
            "step 3000: train loss 1.6136, val loss 1.6301\n",
            "step 3100: train loss 1.6192, val loss 1.6206\n",
            "step 3200: train loss 1.6138, val loss 1.6221\n",
            "step 3300: train loss 1.5953, val loss 1.6125\n",
            "step 3400: train loss 1.6152, val loss 1.6146\n",
            "step 3500: train loss 1.5969, val loss 1.6095\n",
            "step 3600: train loss 1.5849, val loss 1.5983\n",
            "step 3700: train loss 1.5836, val loss 1.5996\n",
            "step 3800: train loss 1.5791, val loss 1.5939\n",
            "step 3900: train loss 1.5705, val loss 1.5871\n",
            "step 4000: train loss 1.5749, val loss 1.5880\n",
            "step 4100: train loss 1.5689, val loss 1.5861\n",
            "step 4200: train loss 1.5539, val loss 1.5662\n",
            "step 4300: train loss 1.5473, val loss 1.5877\n",
            "step 4400: train loss 1.5471, val loss 1.5679\n",
            "step 4500: train loss 1.5542, val loss 1.5673\n",
            "step 4600: train loss 1.5527, val loss 1.5537\n",
            "step 4700: train loss 1.5379, val loss 1.5644\n",
            "step 4800: train loss 1.5301, val loss 1.5404\n",
            "step 4900: train loss 1.5383, val loss 1.5525\n",
            "step 4999: train loss 1.5322, val loss 1.5453\n",
            "\n",
            "think they pocked .\n",
            "She stark was saw they suffed his have some light .\n",
            "Pratherhaus that it Ockhoously .\n",
            "Harry they's loquid he feen the large to mischen caused to bell eyever or : \" U fire stall the experses at He canatet somewhered that would give the snipped where being and the at they not dreathed pupe feeling ? \"\n",
            "he's they olds Koney of him ) and showever obes theirge off him Harry , \" said Ron , Ind DonGfing Bagaguming solly .\n",
            "\" Sorten it , Neville of some twell appared aion shuffed home poked werebed .\n",
            "He seen Neving the freed attere upsather to decizard that of mean , met remong whille was gattened you courses witwas againstaustly at at the mum some , shopping and smept , and beds their was clests into the momes but have slaped wideous at their stang to moment the d best matched him ! \"\n",
            "He caused an mared to peetly book only , \" meet I'd no shoping at the omeled kef eyes , \" \" said grimped Half , eyes on \" \" \" Harry .\n",
            "\" Whippit Dumbledores .\n",
            "\" Jum Chopt osk are relittend he was felled at light ready ask : To tholl Dall ! \"\n",
            "\" Harry , no , Headf oven him looked own dix medtear , the nesty mut that onclot Phofe these to ho up some of where ? \"\n",
            "Harry hers insto the mome someboth , coutely how the sure wound emberved nobe that he'd be pake , standing again , courserathing figive forady all watched he know they waddered Harry , hoy a thike himself that moment , what made is heed pass going .\n",
            "\" What contersed of they elf und Knatwitch .\n",
            "As the good if gordly sitched to wind .\n",
            "Azka to farthen bark , stust some withorour habs crumbut she for hand the laster .\n",
            "\" Mhis Bismame and romember opib and gante maggicatessed and deming eld , and up saw .\n",
            "\" It's more dum at them jamffent the greelass .\n",
            "Werew seames on before into the shraicortift hom .\n",
            "The chatfoy in voice hallip nor ev gloous's their .\n",
            "\" Professor Mc'cnocked , \" \" and should heaving able showled the lipted quest and Harry and her fressed yover of with gat to meake . \"\n",
            "\" Y've got . \"\n",
            "\" Hadn't the Yeen about onl\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('./HarryPotterTxTFile.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 残差链接\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
